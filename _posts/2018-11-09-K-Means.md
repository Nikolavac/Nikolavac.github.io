---
layout: post
title:  "K-Means"
date:   2018-11-09 11:24:31 +0800
author: Nikolavac
categories: Clustering 
math: y
published: true

---
### **K-MEANS**
---
### _K-Means clustering_

>&#160; &#160; &#160; &#160;[k-Means](https://en.wikipedia.org/wiki/K-means_clustering)算法是一种原型聚类方法。对于给定的数据集，将样本分割为k个类别。聚类划为的目标函数为：

$$E=\sum_{i=1}^{k}{\sum_{x \varepsilon C_{i}}{\left \| X-U_{i} \right \| _{2}^{2}  }}$$

$$U_{i} = \frac{1}{\left | C_{i} \right |}\sum_{x \varepsilon C_{i}}{X}$$

>&#160; &#160; &#160; &#160;其中，X是样本，$U_{i}$ 为某一类别中的均值位置。最小化均方误差函数，均方误差的函数在某种程度上描述了簇内的样本与均值的紧密程度。E值的越小，则表明簇内样本的相似性越高。最小均方误差函数所使用的度量方式为欧式距离，优化目标函数是一个N-P难问题。故聚类结果的好坏与初值选取有着较大的关系。

###  _law of total variance_ 

>&#160; &#160; &#160; &#160;在概率理论中，若X，Y是同一概率空间中的随机变量，那么Y的方差存在且存在如下的条件方差分解公式：

$$Var(Y) = E[Var(Y|X)] + Var(E[Y|X])$$

>第一项称为是 _the expected value of the process variance (EVPV)_ 第一部分与X相关部分的方差,而第二项称为是 _the variance of the hypothetical means (VHM)_ 是与X无关的部分(残差)方差。这两项不相关。




### _Equal Formulation_


>&#160; &#160; &#160; &#160;优化的方式可以理解为：最小化簇内的样本间的欧式距离，或是最小化所有簇内的方差和。其中$S_{i}$是第i个簇。

$$arg min \sum_{i=1}^{k}{\sum_{x \varepsilon C_{i}}{\left \| X-U_{i} \right \| _{2}^{2}  }} = 
    \sum_{i=1}^{k}{\left | S_{i} \right | Var S_{i}}$$

>&#160; &#160; &#160; &#160;也可以写成如下形式：

$$arg min \sum_{i=1}^{k}\frac{1}{2\left | S_{i} \right |} \sum_{x,y\varepsilon S_{i}}{\left \| X-Y \right \|^{2}}$$

>&#160; &#160; &#160; &#160;上式用于描述在同一簇内的任意两个点间的平方误差。由全方差法则(与全期望公式一致) _(law of total variance)_ 总的方差不变性。所以上式中最小化簇内方差等价于最小化任意两个在同一簇内的两个样本间的平方误差。亦可以通过最大化不同簇间点的平方误差和来优化目标函数。


### _Procedure_

> 1. 在全部样本中随机选择k个中心点，计算每一个样本点到k个中心点的距离。

> 2. 划分集合，计算集合内的平均位置(质心)。

> 3. 迭代固定次数或中心点位置两次迭代间变化小于某数值

### _Drawback_

- K-Means算法使用欧式距离作为度量的方式，方差作为度量簇内散度的方式。
- 聚类个数k是作为算法的输入参数
- 使用贪婪搜索，结果是一个局部极值将会产生违反直接的不实际结果。

> >&#160; &#160; &#160; &#160;例如在[Iris数据集](http://archive.ics.uci.edu/ml/datasets/Iris)合中，使用k-means算法，当设置k=3时，聚类总是失败的。总是存在两个属性被均匀的分隔为2个簇内。而当k=2时，总是可以发现数据集被分为两个较明显的簇。对于Iris数据集，k-means算法对应的簇的个数应该为2，即使数据集的样本是3个类别。这是由于k-means算法使用的是球形聚类的观点去分类数据，它总是希望聚类后的簇大小是相近的。

>>&#160; &#160; &#160; &#160;我们可以基于k-means算法对结果做出一些假设，正如其他的算法一样，k-means算法并不是万能，在某些数据集上，它的表现并不好。对于Iris数据集，最大期望算法('_EM_')

### Advance
1. K-Means++
2. Fuzzy clustering
>&#160; &#160; &#160; &#160;模糊聚类可以构成数据点不只属于单一类别的情况。聚类的思想是使相似的数据尽可能的在一个簇内。使不相似的数据尽可能不在同一个簇内。相比与k-Means算法，K-Means算法只考虑了类内的相似度。这里面所谓的相似性包括，距离、连通性还有密集程度。依据不同的数据集进行选择。
>在非模糊的聚类模式中，数据将会被强制分配在固定的簇中。每一个样本只能属于某一个类别。最为广泛使用的模糊聚类方法为  '_FCM_'&#160; &#160;(_Fuzzy C-means clustering Algorithm_')。FCM算法的步骤与K-means一致。区别在与


### Applications
>&#160; &#160; &#160; &#160;



### _Python Code_

~~~python
import numpy as np
from bokeh.plotting import figure, gridplot, show, output_notebook


#==============================================================================#

def kmeans(points, n_clusters):
    # sample initial centroids
    '''随机选择中心点'''
    sample = np.random.choice(
        len(points), n_clusters, replace=False
    )
    '''确定中心点坐标'''
    #print(sample)
    centroid = points[sample]
    #print(centroid)
    
    loss = [-1, -2]
    '''allclose 返回bool型 当两个数组的元素相等（tolerance）'''
    while not np.allclose(*loss):
        # compute distance for each pair: point/centroid
        '''
        distance 为list类型，长度为聚类个数
        list内的每一个元素为全部点到某一个中心的欧式距离
        [ [300*1],[300*1],[300*1] ]
        '''
        distance = [
            np.sqrt(((points - c) ** 2).sum(1)) for c in centroid
        ]

        print(distance)
        #print(len(distance))
        # new loss
        '''更新损失'''
        '''也可以计算全部的欧式距离误差'''
        print(np.sum(distance))
        loss = loss[1:] + [np.sum(distance)]
        # assign new clusters
        cluster = np.argmin(distance, axis=0)
        # update centroids by new cluster means
        for i in range(n_clusters):
            centroid[i] = np.mean(points[cluster == i], axis=0)
        
    return cluster


#==============================================================================#

'''初始化'''
n = 2
A = np.random.multivariate_normal([2, 0], [[1, .1], [-4, 1]], n)
B = np.random.multivariate_normal([-2, 0], [[1, -4], [.1, 1]], n)
C = np.random.multivariate_normal([2, -2], [[1, 4], [-.1, 1]], n)
D = ['red', 'green', 'blue']


#==============================================================================#
'''np.r_ 合并数组 points.shape = (300,2)'''
points = np.r_[A, B, C]
#print(points.shape)
'''original_color 是ndarray的数组，shape = (300,)'''
original_color = np.repeat(D[:3], n)
#print(type(original_color))
#print(original_color.shape)
#print(original_color)
'''聚类'''
cluster = kmeans(points, 3)

#==============================================================================#
'''画图'''
new_color = [D[i] for i in cluster]

output_notebook()

plot1 = figure(title='original clusters', plot_height=300)
plot1.scatter(x=points[:, 0], y=points[:, 1], color=original_color)

plot2 = figure(title='k-means clusters', plot_height=300)
plot2.scatter(x=points[:, 0], y=points[:, 1], color=new_color)

show(gridplot([[plot1], [plot2]]))


~~~

>
